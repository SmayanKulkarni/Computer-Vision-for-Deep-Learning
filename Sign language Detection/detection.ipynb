{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f988d58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 10:45:16.164213: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-23 10:45:16.287346: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753247716.332538    4507 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753247716.345902    4507 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753247716.455847    4507 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753247716.455864    4507 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753247716.455867    4507 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753247716.455868    4507 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-23 10:45:16.467710: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import cv as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82fd27b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_face_mesh = mp.solutions.face_mesh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4da0f3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv.cvtColor(image, cv.COLOR_RGB2BGR)\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13a3fa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, result):\n",
    "    if result.face_landmarks:\n",
    "        mp_drawing.draw_landmarks(image, result.face_landmarks, mp_face_mesh.FACEMESH_TESSELATION)\n",
    "    if result.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(image, result.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "    if result.left_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(image, result.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "    if result.right_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(image, result.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88e6cba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1579e670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1753247718.441606    4507 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1753247718.507087    7024 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 570.172.08), renderer: NVIDIA GeForce RTX 4070 SUPER/PCIe/SSE2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1753247718.551707    7000 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753247718.571255    7001 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753247718.573411    7002 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753247718.573514    6997 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753247718.574086    7013 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753247718.580248    7005 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753247718.582429    7022 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "W0000 00:00:1753247718.583326    7014 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753247718.585689    7007 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "cap = cv.VideoCapture('sign.mp4')\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Video ended or cannot read the frame.\")\n",
    "            break\n",
    "\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "            \n",
    "\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        cv.imshow('OpenCV feed', image)\n",
    "        if cv.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbe51015",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.right_hand_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b2eb958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[132, 130, 134],\n",
       "        [128, 126, 130],\n",
       "        [133, 131, 135],\n",
       "        ...,\n",
       "        [114, 118, 125],\n",
       "        [114, 118, 125],\n",
       "        [114, 118, 125]],\n",
       "\n",
       "       [[130, 128, 132],\n",
       "        [130, 128, 132],\n",
       "        [135, 133, 137],\n",
       "        ...,\n",
       "        [114, 118, 125],\n",
       "        [114, 118, 125],\n",
       "        [114, 118, 125]],\n",
       "\n",
       "       [[129, 127, 131],\n",
       "        [133, 131, 135],\n",
       "        [136, 134, 138],\n",
       "        ...,\n",
       "        [114, 118, 125],\n",
       "        [114, 118, 125],\n",
       "        [114, 118, 125]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[164, 164, 158],\n",
       "        [162, 162, 156],\n",
       "        [162, 162, 156],\n",
       "        ...,\n",
       "        [ 83,  96, 116],\n",
       "        [ 84,  97, 116],\n",
       "        [ 83,  96, 115]],\n",
       "\n",
       "       [[157, 157, 151],\n",
       "        [158, 158, 152],\n",
       "        [162, 162, 156],\n",
       "        ...,\n",
       "        [ 83,  96, 116],\n",
       "        [ 83,  96, 115],\n",
       "        [ 82,  95, 114]],\n",
       "\n",
       "       [[150, 150, 144],\n",
       "        [153, 153, 147],\n",
       "        [158, 158, 152],\n",
       "        ...,\n",
       "        [ 83,  96, 116],\n",
       "        [ 83,  96, 115],\n",
       "        [ 81,  94, 113]]], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fbfce9",
   "metadata": {},
   "source": [
    "## Extracting Keypoint Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b33ad86f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x: 0.267064571\n",
       "y: 0.373800635\n",
       "z: -1.2376591\n",
       "visibility: 0.999559462"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.pose_landmarks.landmark[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9e8b1c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26706457138061523"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.pose_landmarks.landmark[0].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4935a965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3738006353378296"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.pose_landmarks.landmark[0].y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95585882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.2376590967178345"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.pose_landmarks.landmark[0].z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "109d90c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = []\n",
    "\n",
    "for res in results.pose_landmarks.landmark:\n",
    "   test = np.array([res.x, res.y, res.z, res.visibility])\n",
    "   pose.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f8f0809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.26706457,  0.37380064, -1.2376591 ,  0.99955946]),\n",
       " array([ 0.32199806,  0.33850104, -1.26252556,  0.99942458]),\n",
       " array([ 0.34669897,  0.33909333, -1.26285565,  0.99947667]),\n",
       " array([ 0.37435278,  0.34005755, -1.26354063,  0.99954122]),\n",
       " array([ 0.27456048,  0.34040743, -1.12301517,  0.99911761]),\n",
       " array([ 0.26380023,  0.34214309, -1.12236619,  0.99908227]),\n",
       " array([ 0.25443962,  0.34368473, -1.12325418,  0.99896157]),\n",
       " array([ 0.47343865,  0.36998665, -1.05041969,  0.9996441 ]),\n",
       " array([ 0.31070083,  0.36960784, -0.45877403,  0.99903291]),\n",
       " array([ 0.31194615,  0.41558728, -1.17136073,  0.99979192]),\n",
       " array([ 0.25505346,  0.41499743, -0.98817545,  0.99955064]),\n",
       " array([ 0.76034522,  0.62633032, -0.89239115,  0.99897307]),\n",
       " array([ 0.14385974,  0.60643607, -0.12092006,  0.99661833]),\n",
       " array([ 0.89377958,  0.91359901, -0.99031186,  0.58975136]),\n",
       " array([-0.00503417,  0.85288489, -0.08946221,  0.24279097]),\n",
       " array([ 1.01304817,  1.17175007, -1.33636642,  0.10235522]),\n",
       " array([-0.23299524,  1.01083064, -1.06091475,  0.03144431]),\n",
       " array([ 1.07294893,  1.24329221, -1.48294985,  0.11344878]),\n",
       " array([-0.30500123,  1.06059456, -1.23622119,  0.04251609]),\n",
       " array([ 1.01934016,  1.25021684, -1.62427568,  0.17987795]),\n",
       " array([-0.2699497 ,  1.06108296, -1.40585697,  0.07171161]),\n",
       " array([ 0.98069322,  1.22101855, -1.40843463,  0.17133594]),\n",
       " array([-0.23123135,  1.04504764, -1.18173671,  0.0809466 ]),\n",
       " array([ 0.65500426,  1.15382242, -0.23678824,  0.00487278]),\n",
       " array([0.2366329 , 1.14318609, 0.24466029, 0.00446602]),\n",
       " array([ 6.02219641e-01,  1.57279694e+00, -3.44364792e-01,  5.42390626e-04]),\n",
       " array([1.98764652e-01, 1.55147696e+00, 9.76989865e-02, 2.10816899e-04]),\n",
       " array([5.82786322e-01, 1.95174849e+00, 5.91115773e-01, 3.59941114e-05]),\n",
       " array([1.83986068e-01, 1.92952037e+00, 6.66343451e-01, 1.19894694e-05]),\n",
       " array([6.02494001e-01, 2.01258922e+00, 6.28074527e-01, 2.45342799e-05]),\n",
       " array([1.79333180e-01, 1.98692894e+00, 6.87156022e-01, 2.88887077e-05]),\n",
       " array([ 4.66618568e-01,  2.04898119e+00, -3.15111071e-01,  3.44086911e-05]),\n",
       " array([ 1.71693206e-01,  2.03805923e+00, -2.89582312e-01,  3.16979786e-05])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a8db534",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = np.array(pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0259c287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pose.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fd6545e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pose.flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "948c2f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = pose.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae212058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pose.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8999e686",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n",
    "face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n",
    "lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "510f20b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d18e973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa96fea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1662,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_keypoints(results).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1715ece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('MP_Data') \n",
    "\n",
    "# Actions that we try to detect\n",
    "actions = np.array(['hello', 'thanks', 'iloveyou'])\n",
    "\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 30\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 30\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f937c438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected actions: ['bye' 'copy' 'disgust' 'eternity' 'hello' 'human' 'love' 'thanks']\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = 'MP_Data'\n",
    "\n",
    "# Automatically gather class labels from folder names\n",
    "actions = np.array(sorted([folder for folder in os.listdir(DATA_PATH) if os.path.isdir(os.path.join(DATA_PATH, folder))]))\n",
    "print(\"Detected actions:\", actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17005f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "#     for action in actions:\n",
    "#         action_path = os.path.join(DATA_PATH, action)\n",
    "#         video_files = [f for f in os.listdir(action_path) if f.endswith('.mp4')]\n",
    "\n",
    "#         for sequence, video_file in enumerate(video_files):\n",
    "#             cap = cv.VideoCapture(os.path.join(action_path, video_file))\n",
    "#             frame_num = 0\n",
    "\n",
    "#             while cap.isOpened() and frame_num < sequence_length:\n",
    "#                 ret, frame = cap.read()\n",
    "#                 if not ret:\n",
    "#                     break\n",
    "\n",
    "#                 _, results = mediapipe_detection(frame, holistic)\n",
    "#                 keypoints = extract_keypoints(results)\n",
    "\n",
    "#                 # Save keypoints as .npy\n",
    "#                 sequence_dir = os.path.join(DATA_PATH, action, str(sequence))\n",
    "#                 os.makedirs(sequence_dir, exist_ok=True)\n",
    "#                 npy_path = os.path.join(sequence_dir, f'{frame_num}.npy')\n",
    "#                 np.save(npy_path, keypoints)\n",
    "\n",
    "#                 frame_num += 1\n",
    "#             cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e964f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label: num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "728d2bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bye': 0,\n",
       " 'copy': 1,\n",
       " 'disgust': 2,\n",
       " 'eternity': 3,\n",
       " 'hello': 4,\n",
       " 'human': 5,\n",
       " 'love': 6,\n",
       " 'thanks': 7}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83b83d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33afc3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in range(4):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1fbba424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 30, 1662)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sequences).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54c82200",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e34b3ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a63e23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e7523ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, RNN, Bidirectional, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2628ee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d42a36bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1753247720.490810    4507 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9116 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 SUPER, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "/home/smayan/Desktop/AI-ML-DS/AI-and-ML-Course/.conda/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1662)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "adba0450",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "86a16246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">442,112</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">264</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │       \u001b[38;5;34m442,112\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m98,816\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m49,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │           \u001b[38;5;34m264\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">596,840</span> (2.28 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m596,840\u001b[0m (2.28 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">596,840</span> (2.28 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m596,840\u001b[0m (2.28 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4d077ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X_train, y_train, epochs=500, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "836e0522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">442,112</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">264</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │       \u001b[38;5;34m442,112\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m98,816\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m49,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │           \u001b[38;5;34m264\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">596,840</span> (2.28 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m596,840\u001b[0m (2.28 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">596,840</span> (2.28 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m596,840\u001b[0m (2.28 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6b119494",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('action2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d4c5d1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('action2.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e54a323a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model('action.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "529dc500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">884,224</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">41,216</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">264</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m884,224\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m41,216\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │           \u001b[38;5;34m264\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">927,786</span> (3.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m927,786\u001b[0m (3.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">927,784</span> (3.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m927,784\u001b[0m (3.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5a21e97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('action.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "09ab89b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1753248187.517251    7116 service.cc:152] XLA service 0x7f82b0001cc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1753248187.517268    7116 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 SUPER, Compute Capability 8.9\n",
      "2025-07-23 10:53:07.545343: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1753248187.658708    7116 cuda_dnn.cc:529] Loaded cuDNN version 91100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 637ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1753248187.958054    7116 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    }
   ],
   "source": [
    "res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d738e45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.9059086e-02, 1.3626700e-02, 2.5284049e-01, 2.5411421e-01,\n",
       "        8.2307644e-02, 2.7819473e-01, 2.2096965e-02, 7.7760160e-02],\n",
       "       [1.0660870e-10, 1.3511191e-01, 7.5052892e-05, 5.2560239e-08,\n",
       "        2.6089751e-06, 1.8602709e-01, 6.7878276e-01, 4.5567529e-07],\n",
       "       [8.8261318e-01, 6.0613627e-05, 1.0983580e-01, 5.2947975e-03,\n",
       "        1.0672770e-07, 4.7394878e-06, 3.1562660e-05, 2.1591443e-03],\n",
       "       [1.5745857e-01, 4.8438224e-04, 5.2396744e-01, 2.5058228e-01,\n",
       "        6.6656474e-04, 3.2238713e-03, 2.0909614e-03, 6.1525945e-02]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "43b864da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01905909, 0.0136267 , 0.2528405 , 0.2541142 , 0.08230764,\n",
       "       0.27819473, 0.02209697, 0.07776016], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8695e246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9772cfe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "56c8be4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'disgust'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(res[3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8e5e62af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'disgust'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(y_test[3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "91183262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "74de3c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n"
     ]
    }
   ],
   "source": [
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3079114",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "106076b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[3, 0],\n",
       "        [0, 1]],\n",
       "\n",
       "       [[3, 0],\n",
       "        [0, 1]],\n",
       "\n",
       "       [[3, 1],\n",
       "        [0, 0]],\n",
       "\n",
       "       [[3, 0],\n",
       "        [0, 1]],\n",
       "\n",
       "       [[3, 0],\n",
       "        [1, 0]]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "725de1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c27e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv.putText(output_frame, actions[num], (0, 85+num*40), cv.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8a9e3385",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only length-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m18\u001b[39m,\u001b[38;5;241m18\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mprob_viz\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolors\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[75], line 5\u001b[0m, in \u001b[0;36mprob_viz\u001b[0;34m(res, actions, input_frame, colors)\u001b[0m\n\u001b[1;32m      3\u001b[0m output_frame \u001b[38;5;241m=\u001b[39m input_frame\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m num, prob \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(res):\n\u001b[0;32m----> 5\u001b[0m     cv\u001b[38;5;241m.\u001b[39mrectangle(output_frame, (\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m60\u001b[39m\u001b[38;5;241m+\u001b[39mnum\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m40\u001b[39m), (\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprob\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;241m90\u001b[39m\u001b[38;5;241m+\u001b[39mnum\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m40\u001b[39m), colors[num], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m     cv\u001b[38;5;241m.\u001b[39mputText(output_frame, actions[num], (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m85\u001b[39m\u001b[38;5;241m+\u001b[39mnum\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m40\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m255\u001b[39m,\u001b[38;5;241m255\u001b[39m,\u001b[38;5;241m255\u001b[39m), \u001b[38;5;241m2\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mLINE_AA)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_frame\n",
      "\u001b[0;31mTypeError\u001b[0m: only length-1 arrays can be converted to Python scalars"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x1800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(18,18))\n",
    "plt.imshow(prob_viz(res, actions, image, colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3fb2a2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@1674.971] global cap_v4l.cpp:913 open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n",
      "[ERROR:0@1674.972] global obsensor_uvc_stream_channel.cpp:158 getStreamChannelGroup Camera index out of range\n",
      "I0000 00:00:1753249390.487355    4507 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1753249390.528703   15628 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 570.172.08), renderer: NVIDIA GeForce RTX 4070 SUPER/PCIe/SSE2\n",
      "W0000 00:00:1753249390.563184   15604 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753249390.581075   15617 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753249390.582339   15617 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753249390.582863   15602 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753249390.583091   15613 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753249390.586197   15617 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753249390.587352   15619 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1753249390.591892   15613 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "threshold = 0.8\n",
    "\n",
    "cap = cv.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "#         sequence.insert(0,keypoints)\n",
    "#         sequence = sequence[:30]\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(actions[np.argmax(res)])\n",
    "            \n",
    "            \n",
    "        #3. Viz logic\n",
    "            if res[np.argmax(res)] > threshold: \n",
    "                if len(sentence) > 0: \n",
    "                    if actions[np.argmax(res)] != sentence[-1]:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "                else:\n",
    "                    sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # Viz probabilities\n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "        cv.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv.putText(image, ' '.join(sentence), (3,30), \n",
    "                       cv.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e5dfea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
